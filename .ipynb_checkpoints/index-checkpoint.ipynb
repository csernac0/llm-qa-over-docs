{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5dfc5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.9.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df485255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_document import LoadDocument\n",
    "from src.get_embeddings import GetEmbeddings\n",
    "from src.qa_context import QAWithContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1c5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_dir = 'data_example_code'\n",
    "type_doc = '.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c67e9",
   "metadata": {},
   "source": [
    "## Get document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72ced32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('data_example_code/load_document.py'), PosixPath('data_example_code/get_embeddings.py'), PosixPath('data_example_code/qa_context.py')]\n"
     ]
    }
   ],
   "source": [
    "obj_load_doc = LoadDocument(document_dir, type_doc)\n",
    "lgc_documents = obj_load_doc.get_lgc_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c2f14",
   "metadata": {},
   "source": [
    "## Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f15508",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_embeddings = GetEmbeddings(document_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea7fffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 21\n",
      "Total word count: 1503\n",
      "Total tokens: 3782\n",
      "Embedding Cost: $0.03 MXN\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1000\n",
    "max_chunk_overlap = 500\n",
    "splitted_doc = obj_embeddings.calc_estimated_cost(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca784c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import os\\nimport re\\nimport pathlib\\nfrom langchain.docstore.document import Document\\n\\n\\nclass LoadDocument(object):\\n    def __init__(self, documents_dir, type_of_file):\\n        \"\"\"\\n        Instanciate document dir.\\n        \"\"\"\\n        self.documents_dir = documents_dir\\n        self.type_of_file = type_of_file\\n\\n    def convert_path_to_doc_url(self, doc_name):\\n        \"\"\"\\n        Convert folder and file name into an url.\\n        \\n        Parameters\\n        ----------\\n        doc_name : string\\n            Document name.\\n        document_folder : string\\n            Folder name.\\n        Returns\\n        -------\\n        url_name: string\\n            Url for document.\\n        \"\"\"\\n        # Cast json to dataframe.\\n        # Convert path string into url.\\n        url_name = re.sub(\\n            f\"{self.documents_dir}/(.*)\\\\.[\\\\w\\\\d]+\", f\"/\\\\\\\\1\",\\n            str(doc_name)\\n        )\\n        return url_name', metadata={'source': '/load_document'}),\n",
       " Document(page_content='def get_lgc_documents(self):\\n        \"\"\"\\n        Read txt files located in the given path, \\n        then convert texts into document store langchain.\\n        This object allows us to keep text and metadata as well.', metadata={'source': '/load_document'}),\n",
       " Document(page_content='Returns\\n        -------\\n        lg_documents: langchain.docstore.document.Document\\n            The object has text and metadata of the file as well.\\n        \"\"\"\\n        # Get data path.\\n        data_path = pathlib.Path(\\n            os.path.join(self.documents_dir)\\n        )\\n        # Get all docs in path.\\n        document_f = list(\\n            data_path.glob(\\'**/*\\' + self.type_of_file)\\n        )\\n        print(document_f)\\n        # Convert docs into langchain docs.\\n        lg_documents = [\\n            Document(\\n                page_content=open(file, \"r\").read(),\\n                metadata={\\n                    \"source\": self.convert_path_to_doc_url(\\n                        file\\n                    )\\n                }\\n            )\\n            for file in document_f\\n        ]\\n        return lg_documents', metadata={'source': '/load_document'}),\n",
       " Document(page_content='import os\\nimport logging\\nimport tiktoken\\nimport numpy as np\\nfrom dotenv import load_dotenv, find_dotenv\\n\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\n\\n\\nclass GetEmbeddings(object):\\n    def __init__(self, documents_dir):\\n        \"\"\"\\n        Instanciate OpenAI api key from .env file.\\n        Instanciate documents dir.\\n        \"\"\"\\n        load_dotenv(find_dotenv())\\n        os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\\n        self.documents_dir = documents_dir', metadata={'source': '/get_embeddings'}),\n",
       " Document(page_content='def split_text_chunks(\\n        self, documents, chunk_size_limit, max_chunk_overlap\\n    ):\\n        \"\"\"\\n        Split complete document text into chunks. \\n        The lenght of each chunk is determined by chunk_size_limit.\\n        Also an overlap between chunks is permited.\\n        \\n        Parameters\\n        ----------\\n        documents : langchain.docstore.document.Document\\n            Langchain document object.\\n        chunk_size_limit : int\\n            Lenght of each chunk in characters.\\n        max_chunk_overlap : int\\n            Character overlap.\\n        Returns\\n        -------\\n        splitted_doc: langchain.text_splitter.CharacterTextSplitter\\n            Document into chunks, including metadata.\\n        \"\"\"\\n        # Set logger.\\n        logging.getLogger(\\n            \"langchain.text_splitter\"\\n        ).setLevel(logging.CRITICAL)\\n        # Instanciate text splitter object with desired params.\\n        text_splitter = CharacterTextSplitter(\\n            chunk_size=chunk_size_limit, \\n            chunk_overlap=max_chunk_overlap\\n        )\\n        # Split text into chunks.\\n        splitted_doc = text_splitter.split_documents(\\n            documents\\n        )\\n        print(\\'Splitted doc:\\', len(splitted_doc))\\n        return splitted_doc', metadata={'source': '/get_embeddings'}),\n",
       " Document(page_content='def get_embeddings_st(\\n        self, documents, chunk_size_limit, max_chunk_overlap, dir_to_store\\n    ):\\n        \"\"\"\\n        Create chunks and then get embedding for each chunk. \\n        If embeddings already exists, then load them.\\n        \\n        Parameters\\n        ----------\\n        documents : langchain.docstore.document.Document\\n            Langchain document object.\\n        chunk_size_limit : int\\n            Lenght of each chunk in characters.\\n        max_chunk_overlap : int\\n            Character overlap.\\n        dir_to_store : string\\n            Path to store.\\n        Returns\\n        -------\\n        vector_store: angchain.vectorstores.FAISS\\n            Embedding vectors as FAISS object.\\n        \"\"\"\\n        \\n        # If not exists, create them.\\n        splitted_doc = self.split_text_chunks(\\n            documents, chunk_size_limit, max_chunk_overlap\\n        )\\n        # Use OpenAI embeddings service.\\n        embeddings = OpenAIEmbeddings()\\n        vector_store = FAISS.from_documents(\\n            splitted_doc, embeddings\\n        )\\n        print(\\'Saved\\')\\n        # If path not exists, create it.\\n        if not os.path.isdir(self.documents_dir + \\'/\\' + dir_to_store):\\n            os.makedirs(self.documents_dir + \\'/\\' + dir_to_store)\\n            print(\\n                \"created folder : \", \\n                self.documents_dir + \\'/\\' + dir_to_store\\n            )\\n        # Save embeddings store.\\n        vector_store.save_local(self.documents_dir + \\'/\\' + dir_to_store)\\n        return vector_store', metadata={'source': '/get_embeddings'}),\n",
       " Document(page_content='def get_embeddings(\\n        self, documents, chunk_size_limit, max_chunk_overlap, dir_to_store\\n    ):\\n        \"\"\"\\n        Create chunks and then get embedding for each chunk. \\n        If embeddings already exists, then load them.\\n        \\n        Parameters\\n        ----------\\n        documents : langchain.docstore.document.Document\\n            Langchain document object.\\n        chunk_size_limit : int\\n            Lenght of each chunk in characters.\\n        max_chunk_overlap : int\\n            Character overlap.\\n        dir_to_store : string\\n            Path to store.\\n        Returns\\n        -------\\n        vector_store: angchain.vectorstores.FAISS\\n            Embedding vectors as FAISS object.\\n        \"\"\"\\n        \\n        if os.path.exists(self.documents_dir + \\'/\\' + dir_to_store):\\n            # If vector store exists then load them.\\n            print(\\'Vector store already created.\\')\\n            vector_store = FAISS.load_local(\\n              self.documents_dir + \\'/\\' + dir_to_store,\\n              OpenAIEmbeddings()\\n          )\\n        else:\\n            # If not exists, create them.\\n            splitted_doc = self.split_text_chunks(\\n                documents, chunk_size_limit, max_chunk_overlap\\n            )\\n            # Use OpenAI embeddings service.\\n            embeddings = OpenAIEmbeddings()\\n            vector_store = FAISS.from_documents(\\n                splitted_doc, embeddings\\n            )\\n            print(\\'Saved\\')\\n            # If path not exists, create it.\\n            if not os.path.isdir(self.documents_dir + \\'/\\' + dir_to_store):\\n                os.makedirs(self.documents_dir + \\'/\\' + dir_to_store)\\n                print(\\n                    \"created folder : \", \\n                    self.documents_dir + \\'/\\' + dir_to_store\\n                )\\n            # Save embeddings store.\\n            vector_store.save_local(self.documents_dir + \\'/\\' + dir_to_store)\\n        return vector_store', metadata={'source': '/get_embeddings'}),\n",
       " Document(page_content='def calc_estimated_cost(\\n        self, documents, chunk_size_limit, max_chunk_overlap\\n    ):\\n        \"\"\"\\n        Use this function in case you want to estimate embedding cost.\\n        \\n        Parameters\\n        ----------\\n        documents : langchain.docstore.document.Document\\n            Langchain document object.\\n        chunk_size_limit : int\\n            Lenght of each chunk in characters.\\n        max_chunk_overlap : int\\n            Character overlap.\\n        \"\"\"\\n        splitted_doc = self.split_text_chunks(\\n            documents, chunk_size_limit, max_chunk_overlap, \\n        )\\n        # Create a GPT-4 encoder instance.\\n        enc = tiktoken.encoding_for_model(\"gpt-4\")\\n        # Calculate costs.\\n        total_word_count = sum(\\n            len(doc.page_content.split()) for doc in splitted_doc\\n        )\\n        total_token_count = sum(\\n            len(enc.encode(doc.page_content)) for doc in splitted_doc\\n        )\\n        total_token_cost = np.round(\\n            (total_token_count*0.0004/1000)*18,\\n            2\\n        )\\n        # Print costs.\\n        print(f\"\"\"Total word count: {total_word_count}\"\"\")\\n        print(f\"\"\"Total tokens: {total_token_count}\"\"\")\\n        print(f\"\"\"Embedding Cost: ${total_token_cost} MXN\"\"\")\\n        return [\\n            splitted_doc, total_word_count, total_token_count, total_token_cost\\n        ]', metadata={'source': '/get_embeddings'}),\n",
       " Document(page_content='import os\\nimport logging\\nimport pandas as pd\\nfrom dotenv import load_dotenv\\nfrom IPython.display import display, Markdown\\n\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\\nfrom langchain.chains import RetrievalQAWithSourcesChain\\n\\n\\nclass QAWithContext(object):\\n    def __init__(self, documents_dir):\\n        \"\"\"\\n        Instanciate OpenAI api key from .env file.\\n        Instanciate documents dir.\\n        \"\"\"\\n        self.documents_dir = documents_dir\\n        load_dotenv()\\n        os.environ[\\n            \"OPENAI_API_KEY\"\\n        ] = os.getenv(\"OPENAI_API_KEY\")', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='class QAWithContext(object):\\n    def __init__(self, documents_dir):\\n        \"\"\"\\n        Instanciate OpenAI api key from .env file.\\n        Instanciate documents dir.\\n        \"\"\"\\n        self.documents_dir = documents_dir\\n        load_dotenv()\\n        os.environ[\\n            \"OPENAI_API_KEY\"\\n        ] = os.getenv(\"OPENAI_API_KEY\")\\n\\n    def print_result(self, result):\\n        \"\"\"\\n        Format results.\\n        \\n        Parameters\\n        ----------\\n        result : json\\n            Results of QA. Keys: [\\'question\\', \\'answer\\', \\'source_documents\\']', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='def print_result(self, result):\\n        \"\"\"\\n        Format results.\\n        \\n        Parameters\\n        ----------\\n        result : json\\n            Results of QA. Keys: [\\'question\\', \\'answer\\', \\'source_documents\\']\\n\\n        Returns\\n        -------\\n        output_text : Markdown \\n            Formated text.\\n        \"\"\"\\n        # Format Output.\\n        output_text = f\"\"\"\\n          ### Question: \\n          {result[\\'question\\']}\\n          ### Answer: \\n          {result[\\'answer\\']}\\n          ### All relevant sources:\\n          {\\' \\'.join(list(set([doc.metadata[\\'source\\'] for doc in result[\\'source_documents\\']])))}\\n        \"\"\"\\n        # Display output.\\n        display(Markdown(output_text))', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='Returns\\n        -------\\n        output_text : Markdown \\n            Formated text.\\n        \"\"\"\\n        # Format Output.\\n        output_text = f\"\"\"\\n          ### Question: \\n          {result[\\'question\\']}\\n          ### Answer: \\n          {result[\\'answer\\']}\\n          ### All relevant sources:\\n          {\\' \\'.join(list(set([doc.metadata[\\'source\\'] for doc in result[\\'source_documents\\']])))}\\n        \"\"\"\\n        # Display output.\\n        display(Markdown(output_text))\\n\\n    def embedding_result_similarity(self, vector_store_dir, str_question):\\n        \"\"\"\\n        Returns similarity scores between the stored vector embeddings & \\n        user question.\\n        \\n        Parameters\\n        ----------\\n        vector_store_dir : string\\n            Name of folder where is located vector store.\\n        str_question : string\\n            User question.', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='Returns\\n        -------\\n        vector_store: angchain.vectorstores.FAISS\\n            Embedding vectors as FAISS object.\\n        min_similarity: float\\n            Min similarity score between top 3 more similar documents.\\n        mean_similarity: float\\n            Mean similarity score between top 3 more similar documents.\\n        \"\"\"\\n        # Load embeddings store.\\n        vector_store = FAISS.load_local(\\n            self.documents_dir + \\'/\\' + vector_store_dir,\\n            OpenAIEmbeddings()\\n        )\\n        # Search the chunks with lowest similarity.\\n        search_result = vector_store.similarity_search_with_score(\\n            str_question, k=3\\n        )', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='search_result = pd.DataFrame(search_result)\\n        search_result.columns = [\\'doc\\', \\'similarty\\']\\n        search_result = search_result.sort_values(\\n            by=\\'similarty\\', ascending=True\\n        ).reset_index()\\n        del search_result[\\'index\\']\\n        min_similarity = search_result.similarty.min()\\n        mean_similarity = search_result.head(3).similarty.mean()\\n        return [vector_store, min_similarity, mean_similarity]\\n\\n    def define_vector_store_to_use(self, str_question):\\n        \"\"\"\\n        For two different document chunks it selects the one with \\n        the closest similarity.\\n        \\n        Parameters\\n        ----------\\n        str_question : string\\n            User question.', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='Returns\\n        -------\\n        vector_store : angchain.vectorstores.FAISS \\n            The most appropriate embedding for the given user question.\\n        \"\"\"\\n        (\\n            vector_store_large, min_similarity_large, mean_similarity_large\\n        ) = self.embedding_result_similarity(\\n            \\'vector_store_large_chunk\\', str_question\\n        )\\n        (\\n            vector_store_small, min_similarity_small, mean_similarity_small\\n        ) = self.embedding_result_similarity(\\n            \\'vector_store_small_chunk\\', str_question\\n        )\\n        print(\\n            \\'Min similarity in large chunk embeds:\\', \\n            min_similarity_large\\n        )\\n        print(\\n            \\'Min similarity in small chunk embeds:\\', \\n            min_similarity_small\\n        )\\n        if min_similarity_large<=min_similarity_small:\\n            print(\\'Select large chunk embeds\\')\\n            vector_store = vector_store_large\\n            n_k_args = 4\\n        else:\\n            print(\\'Select small chunk embeds\\')\\n            vector_store = vector_store_small\\n            n_k_args = 8\\n            \\n        return [vector_store, n_k_args]', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='def generate_prompt(self):\\n        \"\"\"\\n        Generate promp for the LLM model.\\n        \\n        Returns\\n        -------\\n        output_text : Markdown \\n            Formated text.\\n        \"\"\"\\n        system_template=\"\"\"\\n            Use the following context \\n            to answer the users question.\\n            Take note of the sources and include \\n            them in the answer in the format: \\n                \"SOURCES: source1 source2\", \\n                use \"SOURCES\" in capital letters regardless \\n                of the number of sources.\\n            If you don\\'t know the answer, \\n            just say that \"I don\\'t know\" \\n            and don\\'t try to make up an answer.\\n            ----------------\\n            {summaries}\\n        \"\"\"\\n        messages = [\\n            SystemMessagePromptTemplate.from_template(\\n                system_template\\n            ),\\n            HumanMessagePromptTemplate.from_template(\\n                \"{question}\"\\n            )\\n        ]\\n        prompt = ChatPromptTemplate.from_messages(messages)\\n        \\n        chain_type_kwargs = {\"prompt\": prompt}\\n        return chain_type_kwargs', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='def ask_chat_gpt(self, str_query, vector_store, n_k_args):\\n        \"\"\"\\n        Use a custom base to fine tune llm OpenAI Chatgpt.\\n        \\n        Parameters\\n        ----------\\n        str_question : string\\n            User question.\\n        vector_store : angchain.vectorstores.FAISS\\n            Embedding vectors as FAISS object.\\n        n_k_args : int\\n            Numer of chunks to use in the fine tunning.', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='Returns\\n        -------\\n        result : json\\n            Results of QA. Keys: [\\'question\\', \\'answer\\', \\'source_documents\\']\\n        \"\"\"\\n        chain_type_kwargs = self.generate_prompt()\\n        \\n        llm = ChatOpenAI(\\n            model_name=\"gpt-3.5-turbo-16k\", \\n            temperature=0, \\n            max_tokens=400\\n        )\\n        \\n        chain = RetrievalQAWithSourcesChain.from_chain_type(\\n            llm=llm,\\n            chain_type=\"stuff\",\\n            retriever=vector_store.as_retriever(\\n                search_kwargs={\"k\": n_k_args},\\n                search_type=\"mmr\"\\n            ),\\n            return_source_documents=True,\\n            chain_type_kwargs=chain_type_kwargs\\n        )\\n        \\n        result = chain(str_query)\\n        return result', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='def ask_chat_gpt_w_multiq(self, str_query, vector_store, n_k_args):\\n        \"\"\"\\n        Use a custom base to fine tune llm OpenAI Chatgpt.\\n        Adittionaly this method generates \\n        Parameters\\n        ----------\\n        str_question : string\\n            User question.\\n        vector_store : angchain.vectorstores.FAISS\\n            Embedding vectors as FAISS object.\\n        n_k_args : int\\n            Numer of chunks to use in the fine tunning.\\n\\n        Returns\\n        -------\\n        result : json\\n            Results of QA. Keys: [\\'question\\', \\'answer\\', \\'source_documents\\']\\n        \"\"\"\\n        chain_type_kwargs = self.generate_prompt()\\n        \\n        llm = ChatOpenAI(\\n            model_name=\"gpt-3.5-turbo-16k\", \\n            temperature=0, \\n            max_tokens=400\\n        )\\n\\n        # llm, and vector_store.as_retriever can be configured.\\n        # Remove all handlers associated with the root logger object.\\n        logging.basicConfig()', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='# llm, and vector_store.as_retriever can be configured.\\n        # Remove all handlers associated with the root logger object.\\n        logging.basicConfig()\\n\\n        for handler in logging.root.handlers[:]:\\n            logging.root.removeHandler(handler)\\n        logging.basicConfig(filename=\\'generated_log.log\\')\\n        logging.getLogger(\\n            \\'langchain.retrievers.multi_query\\'\\n        ).setLevel(logging.INFO)\\n        retriever_from_llm = MultiQueryRetriever.from_llm(\\n                retriever=vector_store.as_retriever(\\n                search_kwargs={\"k\": n_k_args},\\n                search_type=\"mmr\"\\n            ),\\n            llm=llm\\n        )\\n        multi_q_docs = retriever_from_llm.get_relevant_documents(\\n            query=str_query\\n        )\\n\\n\\n        embeddings = OpenAIEmbeddings()\\n        vector_store_multi_q = FAISS.from_documents(\\n            multi_q_docs, embeddings\\n        )', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='embeddings = OpenAIEmbeddings()\\n        vector_store_multi_q = FAISS.from_documents(\\n            multi_q_docs, embeddings\\n        )\\n\\n        chain = RetrievalQAWithSourcesChain.from_chain_type(\\n            llm=llm,\\n            chain_type=\"stuff\",\\n            retriever=vector_store_multi_q.as_retriever(\\n                search_kwargs={\"k\": n_k_args},\\n                search_type=\"mmr\"\\n            ),\\n            return_source_documents=True,\\n            chain_type_kwargs=chain_type_kwargs\\n        )\\n        \\n        result = chain(str_query)\\n        f = open(\"generated_log.log\", \"r\")\\n        log_info = f.read().split(\\n            \\'INFO:langchain.retrievers.multi_query:\\',\\n            1\\n        )[1]\\n        os.remove(\"generated_log.log\")\\n        return [result, log_info]', metadata={'source': '/qa_context'})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33669c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 21\n",
      "Saved\n",
      "created folder :  data_example_code/vector_store_small_chunk\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1000\n",
    "max_chunk_overlap = 500\n",
    "vector_store_small = obj_embeddings.get_embeddings(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap, \n",
    "    'vector_store_small_chunk'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25be9338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 20\n",
      "Total word count: 1733\n",
      "Total tokens: 4388\n",
      "Embedding Cost: $0.03 MXN\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1200\n",
    "max_chunk_overlap = 800\n",
    "vector_store_large = obj_embeddings.calc_estimated_cost(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d00c5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 20\n",
      "Saved\n",
      "created folder :  data_example_code/vector_store_large_chunk\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1200\n",
    "max_chunk_overlap = 800\n",
    "vector_store_large = obj_embeddings.get_embeddings(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap, \n",
    "    'vector_store_large_chunk'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643284ca",
   "metadata": {},
   "source": [
    "## Q&A over document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1151cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_qa_context = QAWithContext(document_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33b4e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.6688988\n",
      "Min similarity in small chunk embeds: 0.65705293\n",
      "Select small chunk embeds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'La data contiene varias funciones y clases que parecen estar relacionadas con la implementación de un sistema de preguntas y respuestas (QA) utilizando el modelo de lenguaje GPT-3.5 de OpenAI. Algunas de las funciones y clases incluyen:\\n\\n1. `QAWithContext`: Esta clase inicializa la clave de la API de OpenAI y el directorio de documentos.\\n\\n2. `generate_prompt`: Esta función genera un mensaje de solicitud para el modelo de lenguaje.\\n\\n3. `ask_chat_gpt`: Esta función utiliza una base personalizada para ajustar el modelo de chat de OpenAI.\\n\\n4. `define_vector_store_to_use`: Esta función selecciona el documento con la mayor similitud a la pregunta del usuario.\\n\\n5. `LoadDocument`: Esta clase se utiliza para cargar documentos desde un directorio especificado.\\n\\n6. `convert_path_to_doc_url`: Esta función convierte la ruta de un documento en una URL.\\n\\n7. `calc_estimated_cost`: Esta función se utiliza para estimar el costo de incrustar un documento.\\n\\n8. `get_lgc_documents`: Esta función lee archivos de texto ubicados en una ruta dada y los convierte en un objeto de almacenamiento de documentos.\\n\\nAdemás, hay varios módulos importados que se utilizan en el código, como `os`, `logging`, `pandas`, `dotenv`, `IPython.display`, `Markdown`, `langchain.embeddings.openai`, `langchain.vectorstores`, `langchain.prompts.chat`, `langchain.chat_models`, `langchain.retrievers.multi_query`, y `langchain.chains`.\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_query = \"Qué contiene esta data?\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt_w_multiq(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "result[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7265ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.5701494\n",
      "Min similarity in small chunk embeds: 0.5700538\n",
      "Select small chunk embeds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'El método `ask_chat_gpt_w_multiq` realiza una consulta a un modelo de lenguaje GPT-4 con contexto y múltiples preguntas. Este método utiliza un modelo de recuperación de información (RetrievalQAWithSourcesChain) para buscar respuestas relevantes en una colección de documentos. \\n\\nPrimero, se utiliza un vector store (FAISS) para almacenar los embeddings de los documentos. Luego, se selecciona el vector store más apropiado para la pregunta del usuario utilizando la función `define_vector_store_to_use`. \\n\\nA continuación, se realiza una consulta al modelo de lenguaje GPT-4 utilizando el contexto y las preguntas proporcionadas. El resultado de la consulta incluye la pregunta, la respuesta y los documentos fuente relevantes. \\n\\nFinalmente, el método formatea y muestra los resultados utilizando la función `print_result`. \\n\\nEn resumen, el método `ask_chat_gpt_w_multiq` realiza una consulta a un modelo de lenguaje GPT-4 con contexto y múltiples preguntas, y muestra los resultados formateados.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_query = \"Qué hace el método ask_chat_gpt_w_multiq?\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt_w_multiq(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "result[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b2585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
