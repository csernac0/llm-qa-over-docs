{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5dfc5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.9.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a591a254",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df485255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_document import LoadDocument\n",
    "from src.get_embeddings import GetEmbeddings\n",
    "from src.qa_context import QAWithContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c1c5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_dir = 'data_example_code'\n",
    "type_doc = '.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c67e9",
   "metadata": {},
   "source": [
    "## Get document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b72ced32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('data_example_code/get_embeddings.py'), PosixPath('data_example_code/qa_context.py')]\n"
     ]
    }
   ],
   "source": [
    "obj_load_doc = LoadDocument(document_dir, type_doc)\n",
    "lgc_documents = obj_load_doc.get_lgc_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c2f14",
   "metadata": {},
   "source": [
    "## Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62f15508",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_embeddings = GetEmbeddings(document_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fea7fffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 14\n",
      "Total word count: 973\n",
      "Total tokens: 2643\n",
      "Embedding Cost: $0.02 MXN\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1000\n",
    "max_chunk_overlap = 500\n",
    "splitted_doc = obj_embeddings.calc_estimated_cost(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ca784c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='import os\\nimport logging\\nimport tiktoken\\nimport numpy as np\\nfrom dotenv import load_dotenv, find_dotenv\\n\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\n\\n\\nclass GetEmbeddings(object):\\n    def __init__(self, documents_dir):\\n        \"\"\"\\n        Instanciate OpenAI api key from .env file.\\n        Instanciate documents dir.\\n        \"\"\"\\n        load_dotenv(find_dotenv())\\n        os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\\n        self.documents_dir = documents_dir', metadata={'source': '/get_embeddings'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33669c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store already created.\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1000\n",
    "max_chunk_overlap = 500\n",
    "vector_store_small = obj_embeddings.get_embeddings(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap, \n",
    "    'vector_store_small_chunk'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25be9338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 14\n",
      "Total word count: 997\n",
      "Total tokens: 2718\n",
      "Embedding Cost: $0.02 MXN\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1200\n",
    "max_chunk_overlap = 800\n",
    "vector_store_large = obj_embeddings.calc_estimated_cost(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0d00c5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store already created.\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1200\n",
    "max_chunk_overlap = 800\n",
    "vector_store_large = obj_embeddings.get_embeddings(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap, \n",
    "    'vector_store_large_chunk'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643284ca",
   "metadata": {},
   "source": [
    "## Q&A over document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1151cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_qa_context = QAWithContext(document_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9de85cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"quienes son los involucrados en el contrato? (incluye nombres)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "33b4e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.6448673\n",
      "Min similarity in small chunk embeds: 0.64997864\n",
      "Select large chunk embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. ¿Cuál es el contenido de esta información?', '2. ¿Qué información se encuentra en esta base de datos?', '3. ¿Qué datos están incluidos en esta fuente de información?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'La data contiene una serie de funciones y clases que se utilizan para realizar preguntas y obtener respuestas utilizando el modelo de lenguaje GPT-3.5-turbo-16k de OpenAI. También se incluyen funciones para cargar y utilizar vectores de palabras y realizar búsquedas en ellos. Además, se proporciona un contexto de conversación para guiar la generación de respuestas.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_query = \"Qué contiene esta data?\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt_w_multiq(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7265ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.4225699\n",
      "Min similarity in small chunk embeds: 0.4225699\n",
      "Select large chunk embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Cuál es la función del método ask_chat_gpt_w_multiq?', '2. Cuáles son las acciones realizadas por el método ask_chat_gpt_w_multiq?', '3. Para qué se utiliza el método ask_chat_gpt_w_multiq?']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "          ### Question: \n",
       "          Qué hace el método ask_chat_gpt_w_multiq?\n",
       "          ### Answer: \n",
       "          El método `ask_chat_gpt_w_multiq` es una función que se utiliza para realizar consultas utilizando el modelo de chat GPT-3.5-turbo-16k de OpenAI. Toma una cadena de consulta, un vector de almacenamiento y un número de argumentos k como argumentos.\n",
       "\n",
       "En este método, se genera un prompt utilizando la función `generate_prompt`, que incluye un mensaje del sistema y un mensaje humano. Luego se crea una instancia del modelo de chat `ChatOpenAI` con el modelo \"gpt-3.5-turbo-16k\" y se configuran los parámetros de temperatura y max_tokens.\n",
       "\n",
       "A continuación, se crea una cadena de recuperación y pregunta y respuesta utilizando la clase `RetrievalQAWithSourcesChain`. Se utiliza el vector de almacenamiento como recuperador y se configuran los parámetros de búsqueda y tipo de búsqueda. También se especifica que se deben devolver los documentos fuente.\n",
       "\n",
       "Finalmente, se llama a la cadena de recuperación con la cadena de consulta y se devuelve el resultado.\n",
       "\n",
       "Fuentes: /untitled\n",
       "          ### All relevant sources:\n",
       "          /untitled\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "str_query = \"Qué hace el método ask_chat_gpt_w_multiq?\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt_w_multiq(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "obj_qa_context.print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f06dfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.33860582\n",
      "Min similarity in small chunk embeds: 0.35162324\n",
      "Select large chunk embeds\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "          ### Question: \n",
       "          Descripción del equipo o maquinaria\n",
       "          ### Answer: \n",
       "          La descripción del equipo o maquinaria es la siguiente:\n",
       "\n",
       "- Marca: Caterpillar\n",
       "- Modelo: 250G\n",
       "- Serie: CCATO950GTAXX02079\n",
       "- Año: 2006\n",
       "- Tipo de equipo: Cargador frontal de accionamiento hidráulico montado sobre ruedas\n",
       "- Estado: Usado\n",
       "- Pedimento número: 22 34 3803 2001554\n",
       "- Ubicación: Ciudad Miguel Alemán, Tamaulipas\n",
       "\n",
       "\n",
       "          ### All relevant sources:\n",
       "          /ocr_text\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "str_query = \"Descripción del equipo o maquinaria\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "obj_qa_context.print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b87ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.33870715\n",
      "Min similarity in small chunk embeds: 0.35162324\n",
      "Select large chunk embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. ¿Cuál es la descripción del equipo o maquinaria?', '2. ¿Puedes darme información sobre el equipo o maquinaria?', '3. ¿Podrías proporcionar detalles acerca del equipo o maquinaria?']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "          ### Question: \n",
       "          Descripción del equipo o maquinaria\n",
       "          ### Answer: \n",
       "          El equipo o maquinaria descrito en el anexo de arrendamiento es un cargador frontal de accionamiento hidráulico montado sobre ruedas. Es usado y tiene el número de pedimento 22 34 3803 2001554. Es de la marca Caterpillar, modelo 250G, serie CCATO950GTAXX02079 y fue fabricado en el año 2006. \n",
       "Source: /ocr_text\n",
       "          ### All relevant sources:\n",
       "          /ocr_text\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "str_query = \"Descripción del equipo o maquinaria\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt_w_multiq(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "obj_qa_context.print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b2585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
