{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad57fe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.9.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df485255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_document import LoadDocument\n",
    "from src.get_embeddings import GetEmbeddings\n",
    "from src.qa_context import QAWithContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c1c5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_dir = 'data_example_code'\n",
    "type_doc = '.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c67e9",
   "metadata": {},
   "source": [
    "## Get document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b72ced32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('data_example_code/get_embeddings.py'), PosixPath('data_example_code/qa_context.py')]\n"
     ]
    }
   ],
   "source": [
    "obj_load_doc = LoadDocument(document_dir, type_doc)\n",
    "lgc_documents = obj_load_doc.get_lgc_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c2f14",
   "metadata": {},
   "source": [
    "## Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62f15508",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_embeddings = GetEmbeddings(document_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fea7fffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 14\n",
      "Total word count: 973\n",
      "Total tokens: 2643\n",
      "Embedding Cost: $0.02 MXN\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1000\n",
    "max_chunk_overlap = 500\n",
    "splitted_doc = obj_embeddings.calc_estimated_cost(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca784c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='import os\\nimport logging\\nimport tiktoken\\nimport numpy as np\\nfrom dotenv import load_dotenv, find_dotenv\\n\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\n\\n\\nclass GetEmbeddings(object):\\n    def __init__(self, documents_dir):\\n        \"\"\"\\n        Instanciate OpenAI api key from .env file.\\n        Instanciate documents dir.\\n        \"\"\"\\n        load_dotenv(find_dotenv())\\n        os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\\n        self.documents_dir = documents_dir', metadata={'source': '/get_embeddings'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "33669c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store already created.\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1000\n",
    "max_chunk_overlap = 500\n",
    "vector_store_small = obj_embeddings.get_embeddings(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap, \n",
    "    'vector_store_small_chunk'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25be9338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 14\n",
      "Total word count: 997\n",
      "Total tokens: 2718\n",
      "Embedding Cost: $0.02 MXN\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1200\n",
    "max_chunk_overlap = 800\n",
    "vector_store_large = obj_embeddings.calc_estimated_cost(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d00c5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector store already created.\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1200\n",
    "max_chunk_overlap = 800\n",
    "vector_store_large = obj_embeddings.get_embeddings(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap, \n",
    "    'vector_store_large_chunk'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643284ca",
   "metadata": {},
   "source": [
    "## Q&A over document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1151cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_qa_context = QAWithContext(document_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33b4e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.6448673\n",
      "Min similarity in small chunk embeds: 0.64963675\n",
      "Select large chunk embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. ¿Cuál es el contenido de esta información?', '2. ¿Qué información se encuentra en esta base de datos?', '3. ¿Qué datos están incluidos en esta fuente de información?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'La data contiene una serie de funciones y clases que se utilizan para realizar preguntas y obtener respuestas utilizando el modelo de lenguaje GPT-3.5-turbo-16k de OpenAI. También se incluyen funciones para cargar y utilizar vectores de palabras y realizar búsquedas en ellos. Además, se proporciona un contexto de conversación para guiar la generación de respuestas.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_query = \"Qué contiene esta data?\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt_w_multiq(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da15cde5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Qué hace el método ask_chat_gpt_w_multiq?',\n",
       " 'answer': 'El método `ask_chat_gpt_w_multiq` es una función que se utiliza para realizar consultas utilizando el modelo de chat GPT-3.5-turbo-16k de OpenAI. Toma una cadena de consulta, un vector de almacenamiento y un número de argumentos k como argumentos.\\n\\nEn este método, se genera un prompt utilizando la función `generate_prompt`, que incluye un mensaje del sistema y un mensaje humano. Luego, se crea una instancia del modelo de chat `ChatOpenAI` con el modelo \"gpt-3.5-turbo-16k\" y se establecen los parámetros de temperatura y max_tokens.\\n\\nA continuación, se crea una cadena de recuperación y pregunta y respuesta utilizando la clase `RetrievalQAWithSourcesChain`. Se utiliza el vector de almacenamiento como recuperador y se especifica el número de argumentos k para la búsqueda. También se configura para devolver los documentos fuente relacionados.\\n\\nFinalmente, se realiza la consulta utilizando la cadena y se devuelve el resultado.\\n\\nFuentes: /untitled',\n",
       " 'sources': '',\n",
       " 'source_documents': [Document(page_content='def ask_chat_gpt(self, str_query, vector_store, n_k_args):\\n        \\n        chain_type_kwargs = self.generate_prompt()\\n        \\n        llm = ChatOpenAI(\\n            model_name=\"gpt-3.5-turbo-16k\", \\n            temperature=0, \\n            max_tokens=400\\n        )\\n        \\n        chain = RetrievalQAWithSourcesChain.from_chain_type(\\n            llm=llm,\\n            chain_type=\"stuff\",\\n            retriever=vector_store.as_retriever(\\n                search_kwargs={\"k\": n_k_args},\\n                search_type=\"mmr\"\\n            ),\\n            return_source_documents=True,\\n            chain_type_kwargs=chain_type_kwargs\\n        )\\n        \\n        result = chain(str_query)\\n        return result\\n\\n    def ask_chat_gpt_w_multiq(self, str_query, vector_store, n_k_args):', metadata={'source': '/untitled'}),\n",
       "  Document(page_content='import os\\nimport logging\\nimport pandas as pd\\nfrom dotenv import load_dotenv\\nfrom IPython.display import display, Markdown\\n\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\\nfrom langchain.chains import RetrievalQAWithSourcesChain\\n\\n\\nclass QAWithContext(object):\\n    def __init__(self):\\n        \"\"\"\\n        Instanciate OpenAI api key.\\n        \"\"\"\\n        load_dotenv()\\n        os.environ[\\n            \"OPENAI_API_KEY\"\\n        ] = os.getenv(\"OPENAI_API_KEY\")\\n\\n    def print_result(self, result):\\n        output_text = f\"\"\"\\n          ### Question: \\n          {result[\\'question\\']}\\n          ### Answer: \\n          {result[\\'answer\\']}\\n          ### All relevant sources:\\n          {\\' \\'.join(list(set([doc.metadata[\\'source\\'] for doc in result[\\'source_documents\\']])))}\\n        \"\"\"\\n        display(Markdown(output_text))', metadata={'source': '/untitled'}),\n",
       "  Document(page_content=\"def define_vector_store_to_use(self, str_question):\\n        (\\n            min_similarity_large, mean_similarity_large\\n        ) = self.embedding_result_similarity(\\n            'vector_store_large_chunk', str_question\\n        )\\n        (\\n            min_similarity_small, mean_similarity_small\\n        ) = self.embedding_result_similarity(\\n            'vector_store_small_chunk', str_question\\n        )\\n        print(\\n            'Min similarity in large chunk embeds:', \\n            min_similarity_large\\n        )\\n        print(\\n            'Min similarity in small chunk embeds:', \\n            min_similarity_small\\n        )\\n        if min_similarity_large<=min_similarity_small:\\n            print('Select large chunk embeds')\\n            embeds_to_use = 'vector_store_large_chunk'\\n            n_k_args = 4\\n        else:\\n            print('Select small chunk embeds')\\n            embeds_to_use = 'vector_store_small_chunk'\\n            n_k_args = 8\\n            \\n        vector_store = FAISS.load_local(\\n            embeds_to_use,\\n            OpenAIEmbeddings()\\n        )\\n        return [vector_store, n_k_args]\", metadata={'source': '/untitled'}),\n",
       "  Document(page_content='def generate_prompt(self):\\n        system_template=\"\"\"\\n            Use the following context \\n            to answer the users question.\\n            Take note of the sources and include \\n            them in the answer in the format: \\n                \"SOURCES: source1 source2\", \\n                use \"SOURCES\" in capital letters regardless \\n                of the number of sources.\\n            If you don\\'t know the answer, \\n            just say that \"I don\\'t know\" \\n            and don\\'t try to make up an answer.\\n            ----------------\\n            {summaries}\\n        \"\"\"\\n        messages = [\\n            SystemMessagePromptTemplate.from_template(\\n                system_template\\n            ),\\n            HumanMessagePromptTemplate.from_template(\\n                \"{question}\"\\n            )\\n        ]\\n        prompt = ChatPromptTemplate.from_messages(messages)\\n        \\n        chain_type_kwargs = {\"prompt\": prompt}\\n        return chain_type_kwargs', metadata={'source': '/untitled'})]}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8308766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.4225699\n",
      "Min similarity in small chunk embeds: 0.4225699\n",
      "Select large chunk embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. Cuál es la función del método ask_chat_gpt_w_multiq?', '2. Cuáles son las acciones realizadas por el método ask_chat_gpt_w_multiq?', '3. Para qué se utiliza el método ask_chat_gpt_w_multiq?']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "          \n",
       "          \n",
       "          ### Question: \n",
       "          Qué hace el método ask_chat_gpt_w_multiq?\n",
       "          ### Answer: \n",
       "          El método `ask_chat_gpt_w_multiq` es una función que se utiliza para realizar consultas utilizando el modelo de chat GPT-3.5-turbo-16k de OpenAI. Toma una cadena de consulta, un vector de almacenamiento y un número de argumentos k como argumentos.\n",
       "\n",
       "En este método, se genera un prompt utilizando la función `generate_prompt`, que incluye un mensaje del sistema y un mensaje humano. Luego, se crea una instancia del modelo de chat `ChatOpenAI` con el modelo \"gpt-3.5-turbo-16k\" y se establecen los parámetros de temperatura y max_tokens.\n",
       "\n",
       "A continuación, se crea una cadena de recuperación y pregunta y respuesta utilizando la clase `RetrievalQAWithSourcesChain`. Se utiliza el vector de almacenamiento como recuperador y se especifica el número de argumentos k para la búsqueda. También se configura para devolver los documentos fuente relacionados.\n",
       "\n",
       "Finalmente, se realiza la consulta utilizando la cadena y se devuelve el resultado.\n",
       "\n",
       "Fuentes: /untitled\n",
       "          ### All relevant sources:\n",
       "          /untitled\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "str_query = \"Qué hace el método ask_chat_gpt_w_multiq?\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt_w_multiq(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "obj_qa_context.print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f06dfed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.33860582\n",
      "Min similarity in small chunk embeds: 0.35162324\n",
      "Select large chunk embeds\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "          ### Question: \n",
       "          Descripción del equipo o maquinaria\n",
       "          ### Answer: \n",
       "          La descripción del equipo o maquinaria es la siguiente:\n",
       "\n",
       "- Marca: Caterpillar\n",
       "- Modelo: 250G\n",
       "- Serie: CCATO950GTAXX02079\n",
       "- Año: 2006\n",
       "- Tipo de equipo: Cargador frontal de accionamiento hidráulico montado sobre ruedas\n",
       "- Estado: Usado\n",
       "- Pedimento número: 22 34 3803 2001554\n",
       "- Ubicación: Ciudad Miguel Alemán, Tamaulipas\n",
       "\n",
       "\n",
       "          ### All relevant sources:\n",
       "          /ocr_text\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "str_query = \"Descripción del equipo o maquinaria\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "obj_qa_context.print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1b87ed72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.33870715\n",
      "Min similarity in small chunk embeds: 0.35162324\n",
      "Select large chunk embeds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:langchain.retrievers.multi_query:Generated queries: ['1. ¿Cuál es la descripción del equipo o maquinaria?', '2. ¿Puedes darme información sobre el equipo o maquinaria?', '3. ¿Podrías proporcionar detalles acerca del equipo o maquinaria?']\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "          ### Question: \n",
       "          Descripción del equipo o maquinaria\n",
       "          ### Answer: \n",
       "          El equipo o maquinaria descrito en el anexo de arrendamiento es un cargador frontal de accionamiento hidráulico montado sobre ruedas. Es usado y tiene el número de pedimento 22 34 3803 2001554. Es de la marca Caterpillar, modelo 250G, serie CCATO950GTAXX02079 y fue fabricado en el año 2006. \n",
       "Source: /ocr_text\n",
       "          ### All relevant sources:\n",
       "          /ocr_text\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "str_query = \"Descripción del equipo o maquinaria\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt_w_multiq(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "obj_qa_context.print_result(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b2585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
