{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5dfc5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.9.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df485255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_document import LoadDocument\n",
    "from src.get_embeddings import GetEmbeddings\n",
    "from src.qa_context import QAWithContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c1c5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_dir = 'data'\n",
    "type_doc = '.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c67e9",
   "metadata": {},
   "source": [
    "## Get document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b72ced32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('data/ocr_text.txt')]\n"
     ]
    }
   ],
   "source": [
    "obj_load_doc = LoadDocument(document_dir, type_doc)\n",
    "lgc_documents = obj_load_doc.get_lgc_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c2f14",
   "metadata": {},
   "source": [
    "## Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62f15508",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_embeddings = GetEmbeddings(document_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fea7fffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 176\n",
      "Total word count: 26539\n",
      "Total tokens: 52994\n",
      "Embedding Cost: $0.38 MXN\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1000\n",
    "max_chunk_overlap = 500\n",
    "splitted_doc = obj_embeddings.calc_estimated_cost(\n",
    "    lgc_documents2, chunk_size_limit, max_chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "ca784c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(splitted_doc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "cf82311c-b601-4848-8a6d-1daddcc6df38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"CONTRATO MAESTRO NUMERO AP000000718 DE ARRENDAMIENTO DE BIENES\\nMUEBLES (EN LO SUCESIVO DENOMINADO EL “ARRENDAMIENTO MAESTRO”) QUE\\nCELEBRAN POR UNA PARTE AB2C LEASING DE MEXICO, SOCIEDAD ANÓNIMA\\nPROMOTORA DE INVERSIÓN DE CAPITAL VARIABLE. (EL “ARRENDADOR'),\\nREPRESENTADA POR MARÍA ISABEL BOLIO MONTERO Y PABLO ENRIQUE ROMERO\\nGONZÁLEZ , POR OTRA PARTE LA EMPRESA CRANE SUPPLIES SERVICES S.A. de\\nC.V. REPRESENTADA POR ÓSCAR ALBERTO ISLAS MENDOZA (“EL ARRENDATARIO”). POR OTRA PARTE: EN LO PERSONAL Y POR SU PROPIO DERECHO, OSCAR\\nALBERTO ISLAS MENDOZA (COMO “EL OBLIGADO SOLIDARIO”), POR ULTIMO EN LO\\nPERSONAL Y POR SU PROPIO DERECHO OSCAR ALBERTO ISLAS MENDOZA, COMO\\n(EL DEPOSITARIO”) DE ACUERDO CON LAS SIGUIENTES DECLARACIONES Y\\nCLAUSULAS.\\n\\nDECLARACIONES\\nE. El Arrendador declara, representa y garantiza que:\", metadata={'source': '/ocr_text'})"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_doc[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "33669c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 121\n",
      "Saved\n",
      "created folder :  data/vector_store_small_chunk\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1000\n",
    "max_chunk_overlap = 500\n",
    "vector_store_small = obj_embeddings.get_embeddings(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap, \n",
    "    'vector_store_small_chunk'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "25be9338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 20\n",
      "Total word count: 1733\n",
      "Total tokens: 4388\n",
      "Embedding Cost: $0.03 MXN\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1200\n",
    "max_chunk_overlap = 800\n",
    "vector_store_large = obj_embeddings.calc_estimated_cost(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d00c5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 20\n",
      "Saved\n",
      "created folder :  data_example_code/vector_store_large_chunk\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1200\n",
    "max_chunk_overlap = 800\n",
    "vector_store_large = obj_embeddings.get_embeddings(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap, \n",
    "    'vector_store_large_chunk'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643284ca",
   "metadata": {},
   "source": [
    "## Q&A over document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1151cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_qa_context = QAWithContext(document_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33b4e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.6688988\n",
      "Min similarity in small chunk embeds: 0.65705293\n",
      "Select small chunk embeds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'La data contiene varias funciones y clases que parecen estar relacionadas con la implementación de un sistema de preguntas y respuestas (QA) utilizando el modelo de lenguaje GPT-3.5 de OpenAI. Algunas de las funciones y clases incluyen:\\n\\n1. `QAWithContext`: Esta clase inicializa la clave de la API de OpenAI y el directorio de documentos.\\n\\n2. `generate_prompt`: Esta función genera un mensaje de solicitud para el modelo de lenguaje.\\n\\n3. `ask_chat_gpt`: Esta función utiliza una base personalizada para ajustar el modelo de chat de OpenAI.\\n\\n4. `define_vector_store_to_use`: Esta función selecciona el documento con la mayor similitud a la pregunta del usuario.\\n\\n5. `LoadDocument`: Esta clase se utiliza para cargar documentos desde un directorio especificado.\\n\\n6. `convert_path_to_doc_url`: Esta función convierte la ruta de un documento en una URL.\\n\\n7. `calc_estimated_cost`: Esta función se utiliza para estimar el costo de incrustar un documento.\\n\\n8. `get_lgc_documents`: Esta función lee archivos de texto ubicados en una ruta dada y los convierte en un objeto de almacenamiento de documentos.\\n\\nAdemás, hay varios módulos importados que se utilizan en el código, como `os`, `logging`, `pandas`, `dotenv`, `IPython.display`, `Markdown`, `langchain.embeddings.openai`, `langchain.vectorstores`, `langchain.prompts.chat`, `langchain.chat_models`, `langchain.retrievers.multi_query`, y `langchain.chains`.\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_query = \"Qué contiene esta data?\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt_w_multiq(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "result[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7265ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.5701494\n",
      "Min similarity in small chunk embeds: 0.5700538\n",
      "Select small chunk embeds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'El método `ask_chat_gpt_w_multiq` realiza una consulta a un modelo de lenguaje GPT-4 con contexto y múltiples preguntas. Este método utiliza un modelo de recuperación de información (RetrievalQAWithSourcesChain) para buscar respuestas relevantes en una colección de documentos. \\n\\nPrimero, se utiliza un vector store (FAISS) para almacenar los embeddings de los documentos. Luego, se selecciona el vector store más apropiado para la pregunta del usuario utilizando la función `define_vector_store_to_use`. \\n\\nA continuación, se realiza una consulta al modelo de lenguaje GPT-4 utilizando el contexto y las preguntas proporcionadas. El resultado de la consulta incluye la pregunta, la respuesta y los documentos fuente relevantes. \\n\\nFinalmente, el método formatea y muestra los resultados utilizando la función `print_result`. \\n\\nEn resumen, el método `ask_chat_gpt_w_multiq` realiza una consulta a un modelo de lenguaje GPT-4 con contexto y múltiples preguntas, y muestra los resultados formateados.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_query = \"Qué hace el método ask_chat_gpt_w_multiq?\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt_w_multiq(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "result[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b2585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
