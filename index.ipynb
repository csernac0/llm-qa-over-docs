{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b5dfc5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python 3.9.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df485255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.load_document import LoadDocument\n",
    "from src.get_embeddings import GetEmbeddings\n",
    "from src.qa_context import QAWithContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c1c5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_dir = 'data_example_code'\n",
    "type_doc = '.py'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266c67e9",
   "metadata": {},
   "source": [
    "## Get document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b72ced32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PosixPath('data_example_code/get_embeddings.py'), PosixPath('data_example_code/qa_context.py')]\n"
     ]
    }
   ],
   "source": [
    "obj_load_doc = LoadDocument(document_dir, type_doc)\n",
    "lgc_documents = obj_load_doc.get_lgc_documents()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787c2f14",
   "metadata": {},
   "source": [
    "## Get embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f15508",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_embeddings = GetEmbeddings(document_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fea7fffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 14\n",
      "Total word count: 973\n",
      "Total tokens: 2643\n",
      "Embedding Cost: $0.02 MXN\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1000\n",
    "max_chunk_overlap = 500\n",
    "splitted_doc = obj_embeddings.calc_estimated_cost(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca784c64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='import os\\nimport logging\\nimport tiktoken\\nimport numpy as np\\nfrom dotenv import load_dotenv, find_dotenv\\n\\nfrom langchain.text_splitter import CharacterTextSplitter\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\n\\n\\nclass GetEmbeddings(object):\\n    def __init__(self, documents_dir):\\n        \"\"\"\\n        Instanciate OpenAI api key from .env file.\\n        Instanciate documents dir.\\n        \"\"\"\\n        load_dotenv(find_dotenv())\\n        os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\\n        self.documents_dir = documents_dir', metadata={'source': '/get_embeddings'}),\n",
       " Document(page_content='def split_text_chunks(\\n        self, documents, chunk_size_limit, max_chunk_overlap\\n    ):\\n        \"\"\"\\n        Split complete document text into chunks. \\n        The lenght of each chunk is determined by chunk_size_limit.\\n        Also an overlap between chunks is permited.\\n        \\n        Parameters:\\n        -----------\\n            documents : langchain.docstore.document.Document\\n                Langchain document object.\\n            chunk_size_limit : int\\n                Lenght of each chunk in characters.\\n            max_chunk_overlap : int\\n                Character overlap.\\n        Returns:\\n        -----------\\n            splitted_doc: langchain.text_splitter.CharacterTextSplitter\\n                Document into chunks, including metadata.\\n        \"\"\"\\n        logging.getLogger(\\n            \"langchain.text_splitter\"\\n        ).setLevel(logging.CRITICAL)', metadata={'source': '/get_embeddings'}),\n",
       " Document(page_content=\"text_splitter = CharacterTextSplitter(\\n            chunk_size=chunk_size_limit, \\n            chunk_overlap=max_chunk_overlap\\n        )\\n\\n        splitted_doc = text_splitter.split_documents(\\n            documents\\n        )\\n        print('Splitted doc:', len(splitted_doc))\\n        return splitted_doc\", metadata={'source': '/get_embeddings'}),\n",
       " Document(page_content='def get_embeddings(\\n        self, documents, chunk_size_limit, max_chunk_overlap, dir_to_store\\n    ):\\n        \"\"\"\\n        Create chunks and then get embedding for each chunk. \\n        If embeddings already exists, then load them.\\n        \\n        Parameters:\\n        -----------\\n            documents : langchain.docstore.document.Document\\n                Langchain document object.\\n            chunk_size_limit : int\\n                Lenght of each chunk in characters.\\n            max_chunk_overlap : int\\n                Character overlap.\\n            dir_to_store : string\\n                Path to store.\\n        Returns:\\n        -----------\\n            vector_store: angchain.vectorstores.FAISS\\n                Embedding vectors as FAISS object.\\n        \"\"\"\\n        \\n        if os.path.exists(self.documents_dir + \\'/\\' + dir_to_store):\\n            # If vector store exists then load them.\\n            print(\\'Vector store already created.\\')\\n            vector_store = FAISS.load_local(\\n              self.documents_dir + \\'/\\' + dir_to_store,\\n              OpenAIEmbeddings()\\n          )\\n        else:\\n            # If not exists, create them.\\n            splitted_doc = self.split_text_chunks(\\n                documents, chunk_size_limit, max_chunk_overlap\\n            )\\n            # Use OpenAI embeddings service.\\n            embeddings = OpenAIEmbeddings()\\n            vector_store = FAISS.from_documents(\\n                splitted_doc, embeddings\\n            )\\n            print(\\'Saved\\')\\n            if not os.path.isdir(self.documents_dir + \\'/\\' + dir_to_store):\\n                os.makedirs(self.documents_dir + \\'/\\' + dir_to_store)\\n                print(\\n                    \"created folder : \", \\n                    self.documents_dir + \\'/\\' + dir_to_store\\n                )\\n            vector_store.save_local(self.documents_dir + \\'/\\' + dir_to_store)\\n        return vector_store', metadata={'source': '/get_embeddings'}),\n",
       " Document(page_content='def calc_estimated_cost(\\n        self, documents, chunk_size_limit, max_chunk_overlap\\n    ):\\n        \"\"\"\\n        Use this function in case you want to estimate embedding cost.\\n        \\n        Parameters:\\n        -----------\\n            documents : langchain.docstore.document.Document\\n                Langchain document object.\\n            chunk_size_limit : int\\n                Lenght of each chunk in characters.\\n            max_chunk_overlap : int\\n                Character overlap.\\n        \"\"\"\\n        splitted_doc = self.split_text_chunks(\\n            documents, chunk_size_limit, max_chunk_overlap, \\n        )\\n        # Create a GPT-4 encoder instance.\\n        enc = tiktoken.encoding_for_model(\"gpt-4\")\\n        total_word_count = sum(\\n            len(doc.page_content.split()) for doc in splitted_doc\\n        )\\n        total_token_count = sum(\\n            len(enc.encode(doc.page_content)) for doc in splitted_doc\\n        )\\n        total_token_cost = np.round(\\n            (total_token_count*0.0004/1000)*18,\\n            2\\n        )\\n        # Print costs.\\n        print(f\"\"\"Total word count: {total_word_count}\"\"\")\\n        print(f\"\"\"Total tokens: {total_token_count}\"\"\")\\n        print(f\"\"\"Embedding Cost: ${total_token_cost} MXN\"\"\")\\n        return splitted_doc', metadata={'source': '/get_embeddings'}),\n",
       " Document(page_content='import os\\nimport logging\\nimport pandas as pd\\nfrom dotenv import load_dotenv\\nfrom IPython.display import display, Markdown\\n\\nfrom langchain.embeddings.openai import OpenAIEmbeddings\\nfrom langchain.vectorstores import FAISS\\nfrom langchain.prompts.chat import (\\n    ChatPromptTemplate,\\n    SystemMessagePromptTemplate,\\n    HumanMessagePromptTemplate,\\n)\\nfrom langchain.chat_models import ChatOpenAI\\nfrom langchain.retrievers.multi_query import MultiQueryRetriever\\nfrom langchain.chains import RetrievalQAWithSourcesChain\\n\\n\\nclass QAWithContext(object):\\n    def __init__(self, documents_dir):\\n        \"\"\"\\n        Instanciate OpenAI api key from .env file.\\n        Instanciate documents dir.\\n        \"\"\"\\n        self.documents_dir = documents_dir\\n        load_dotenv()\\n        os.environ[\\n            \"OPENAI_API_KEY\"\\n        ] = os.getenv(\"OPENAI_API_KEY\")', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='class QAWithContext(object):\\n    def __init__(self, documents_dir):\\n        \"\"\"\\n        Instanciate OpenAI api key from .env file.\\n        Instanciate documents dir.\\n        \"\"\"\\n        self.documents_dir = documents_dir\\n        load_dotenv()\\n        os.environ[\\n            \"OPENAI_API_KEY\"\\n        ] = os.getenv(\"OPENAI_API_KEY\")\\n\\n    def print_result(self, result):\\n        \"\"\"\\n        Format results.\\n        \\n        Parameters:\\n        -----------\\n            result : json\\n                Results of QA. Keys: [\\'question\\', \\'answer\\', \\'source_documents\\']\\n        Returns:\\n        -----------\\n            output_text: Markdown \\n                Formated text.\\n        \"\"\"\\n        output_text = f\"\"\"\\n          ### Question: \\n          {result[\\'question\\']}\\n          ### Answer: \\n          {result[\\'answer\\']}\\n          ### All relevant sources:\\n          {\\' \\'.join(list(set([doc.metadata[\\'source\\'] for doc in result[\\'source_documents\\']])))}\\n        \"\"\"\\n        display(Markdown(output_text))', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='def embedding_result_similarity(self, vector_store_dir, str_question):\\n        \"\"\"\\n        Returns similarity scores for the stored vector embeddings & \\n        user question.\\n        \\n        Parameters:\\n        -----------\\n            vector_store_dir : string\\n                Name of folder where is located vector store.\\n            str_question : string\\n                User question.\\n        Returns:\\n        -----------\\n            vector_store: angchain.vectorstores.FAISS\\n                Embedding vectors as FAISS object.\\n            min_similarity: float\\n                Min similarity score between top 3 more similar documents.\\n            mean_similarity: float\\n                Mean similarity score between top 3 more similar documents.\\n        \"\"\"\\n        vector_store = FAISS.load_local(\\n            self.documents_dir + \\'/\\' + vector_store_dir,\\n            OpenAIEmbeddings()\\n        )\\n        search_result = vector_store.similarity_search_with_score(\\n            str_question, k=3\\n        )', metadata={'source': '/qa_context'}),\n",
       " Document(page_content=\"search_result = pd.DataFrame(search_result)\\n        search_result.columns = ['doc', 'similarty']\\n        search_result = search_result.sort_values(\\n            by='similarty', ascending=True\\n        ).reset_index()\\n        del search_result['index']\\n        min_similarity = search_result.similarty.min()\\n        mean_similarity = search_result.head(3).similarty.mean()\\n        return [vector_store, min_similarity, mean_similarity]\", metadata={'source': '/qa_context'}),\n",
       " Document(page_content='def define_vector_store_to_use(self, str_question):\\n        \"\"\"\\n        For two different document chunks it selects the one with \\n        the closest similarity.\\n        \\n        Parameters:\\n        -----------\\n            str_question : string\\n                User question.\\n        Returns:\\n        -----------\\n            vector_store: angchain.vectorstores.FAISS \\n                The most appropriate embedding for the given user question.\\n        \"\"\"\\n        (\\n            vector_store_large, min_similarity_large, mean_similarity_large\\n        ) = self.embedding_result_similarity(\\n            \\'vector_store_large_chunk\\', str_question\\n        )\\n        (\\n            vector_store_small, min_similarity_small, mean_similarity_small\\n        ) = self.embedding_result_similarity(\\n            \\'vector_store_small_chunk\\', str_question\\n        )\\n        print(\\n            \\'Min similarity in large chunk embeds:\\', \\n            min_similarity_large\\n        )\\n        print(\\n            \\'Min similarity in small chunk embeds:\\', \\n            min_similarity_small\\n        )\\n        if min_similarity_large<=min_similarity_small:\\n            print(\\'Select large chunk embeds\\')\\n            vector_store = vector_store_large\\n            n_k_args = 4\\n        else:\\n            print(\\'Select small chunk embeds\\')\\n            embeds_to_use = vector_store_small\\n            n_k_args = 8\\n            \\n        return [vector_store, n_k_args]', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='def generate_prompt(self):\\n        \"\"\"\\n        Generate promp fro the LLM model.\\n        \\n        Returns:\\n        -----------\\n            output_text: Markdown \\n                Formated text.\\n        \"\"\"\\n        system_template=\"\"\"\\n            Use the following context \\n            to answer the users question.\\n            Take note of the sources and include \\n            them in the answer in the format: \\n                \"SOURCES: source1 source2\", \\n                use \"SOURCES\" in capital letters regardless \\n                of the number of sources.\\n            If you don\\'t know the answer, \\n            just say that \"I don\\'t know\" \\n            and don\\'t try to make up an answer.\\n            ----------------\\n            {summaries}\\n        \"\"\"\\n        messages = [\\n            SystemMessagePromptTemplate.from_template(\\n                system_template\\n            ),\\n            HumanMessagePromptTemplate.from_template(\\n                \"{question}\"\\n            )\\n        ]\\n        prompt = ChatPromptTemplate.from_messages(messages)\\n        \\n        chain_type_kwargs = {\"prompt\": prompt}\\n        return chain_type_kwargs', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='def ask_chat_gpt(self, str_query, vector_store, n_k_args):\\n        \"\"\"\\n        Format results.\\n        \\n        Parameters:\\n        -----------\\n            result : json\\n                Results of QA. Keys: [\\'question\\', \\'answer\\', \\'source_documents\\']\\n        Returns:\\n        -----------\\n            output_text: Markdown \\n                Formated text.\\n        \"\"\"\\n        chain_type_kwargs = self.generate_prompt()\\n        \\n        llm = ChatOpenAI(\\n            model_name=\"gpt-3.5-turbo-16k\", \\n            temperature=0, \\n            max_tokens=400\\n        )\\n        \\n        chain = RetrievalQAWithSourcesChain.from_chain_type(\\n            llm=llm,\\n            chain_type=\"stuff\",\\n            retriever=vector_store.as_retriever(\\n                search_kwargs={\"k\": n_k_args},\\n                search_type=\"mmr\"\\n            ),\\n            return_source_documents=True,\\n            chain_type_kwargs=chain_type_kwargs\\n        )\\n        \\n        result = chain(str_query)\\n        return result', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='def ask_chat_gpt_w_multiq(self, str_query, vector_store, n_k_args):\\n        \"\"\"\\n        Format results.\\n        \\n        Parameters:\\n        -----------\\n            result : json\\n                Results of QA. Keys: [\\'question\\', \\'answer\\', \\'source_documents\\']\\n        Returns:\\n        -----------\\n            output_text: Markdown \\n                Formated text.\\n        \"\"\"\\n        chain_type_kwargs = self.generate_prompt()\\n        \\n        llm = ChatOpenAI(\\n            model_name=\"gpt-3.5-turbo-16k\", \\n            temperature=0, \\n            max_tokens=400\\n        )\\n        \\n        # llm, and vector_store.as_retriever can be configured.\\n        logging.basicConfig()\\n        logging.getLogger(\\n            \\'langchain.retrievers.multi_query\\'\\n        ).setLevel(logging.INFO)\\n        retriever_from_llm = MultiQueryRetriever.from_llm(\\n                retriever=vector_store.as_retriever(\\n                search_kwargs={\"k\": n_k_args},\\n                search_type=\"mmr\"\\n            ),\\n            llm=llm\\n        )\\n        multi_q_docs = retriever_from_llm.get_relevant_documents(\\n            query=str_query\\n        )', metadata={'source': '/qa_context'}),\n",
       " Document(page_content='embeddings = OpenAIEmbeddings()\\n        vector_store_multi_q = FAISS.from_documents(\\n            multi_q_docs, embeddings\\n        )\\n\\n        chain = RetrievalQAWithSourcesChain.from_chain_type(\\n            llm=llm,\\n            chain_type=\"stuff\",\\n            retriever=vector_store_multi_q.as_retriever(\\n                search_kwargs={\"k\": n_k_args},\\n                search_type=\"mmr\"\\n            ),\\n            return_source_documents=True,\\n            chain_type_kwargs=chain_type_kwargs\\n        )\\n        \\n        result = chain(str_query)\\n        return result', metadata={'source': '/qa_context'})]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_doc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33669c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 14\n",
      "Saved\n",
      "created folder :  data_example_code/vector_store_small_chunk\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1000\n",
    "max_chunk_overlap = 500\n",
    "vector_store_small = obj_embeddings.get_embeddings(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap, \n",
    "    'vector_store_small_chunk'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25be9338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 14\n",
      "Total word count: 997\n",
      "Total tokens: 2718\n",
      "Embedding Cost: $0.02 MXN\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1200\n",
    "max_chunk_overlap = 800\n",
    "vector_store_large = obj_embeddings.calc_estimated_cost(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d00c5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted doc: 14\n",
      "Saved\n",
      "created folder :  data_example_code/vector_store_large_chunk\n"
     ]
    }
   ],
   "source": [
    "chunk_size_limit = 1200\n",
    "max_chunk_overlap = 800\n",
    "vector_store_large = obj_embeddings.get_embeddings(\n",
    "    lgc_documents, chunk_size_limit, max_chunk_overlap, \n",
    "    'vector_store_large_chunk'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643284ca",
   "metadata": {},
   "source": [
    "## Q&A over document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1151cce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_qa_context = QAWithContext(document_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9de85cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'quienes son los involucrados en el contrato? (incluye nombres)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"quienes son los involucrados en el contrato? (incluye nombres)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33b4e24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.67280966\n",
      "Min similarity in small chunk embeds: 0.67313385\n",
      "Select large chunk embeds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'La data contiene el código de un programa que utiliza la biblioteca LangChain para realizar preguntas y respuestas utilizando el modelo de lenguaje GPT-3.5 Turbo de OpenAI. El programa incluye funciones para buscar documentos relevantes, calcular la similitud entre documentos, formatear los resultados y estimar el costo de la codificación de texto. También se utilizan otras bibliotecas como pandas, dotenv y IPython.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_query = \"Qué contiene esta data?\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt_w_multiq(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "result[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7265ea3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min similarity in large chunk embeds: 0.5701494\n",
      "Min similarity in small chunk embeds: 0.5700538\n",
      "Select small chunk embeds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'El método `ask_chat_gpt_w_multiq` realiza una consulta a un modelo de lenguaje GPT-4 con contexto y múltiples preguntas. Este método utiliza un modelo de recuperación de información (RetrievalQAWithSourcesChain) para buscar respuestas relevantes en una colección de documentos. \\n\\nPrimero, se utiliza un vector store (FAISS) para almacenar los embeddings de los documentos. Luego, se selecciona el vector store más apropiado para la pregunta del usuario utilizando la función `define_vector_store_to_use`. \\n\\nA continuación, se realiza una consulta al modelo de lenguaje GPT-4 utilizando el contexto y las preguntas proporcionadas. El resultado de la consulta incluye la pregunta, la respuesta y los documentos fuente relevantes. \\n\\nFinalmente, el método formatea y muestra los resultados utilizando la función `print_result`. \\n\\nEn resumen, el método `ask_chat_gpt_w_multiq` realiza una consulta a un modelo de lenguaje GPT-4 con contexto y múltiples preguntas, y muestra los resultados formateados.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str_query = \"Qué hace el método ask_chat_gpt_w_multiq?\"\n",
    "\n",
    "vector_store, n_k_args = obj_qa_context.define_vector_store_to_use(\n",
    "    str_query\n",
    ")\n",
    "\n",
    "result = obj_qa_context.ask_chat_gpt_w_multiq(\n",
    "    str_query, vector_store, n_k_args\n",
    ")\n",
    "\n",
    "result[0]['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6b2585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
